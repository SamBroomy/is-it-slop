{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b24903bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 20 chunks\n",
      "First chunk length: 300 tokens\n",
      "Last chunk length: 251 tokens\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> list[str]:\n",
    "    \"\"\"Split text into overlapping chunks based on tiktoken tokens.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to chunk\n",
    "        chunk_size: Target tokens per chunk\n",
    "        overlap: Overlapping tokens between chunks (helps with boundary effects)\n",
    "\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "\n",
    "    \"\"\"\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "\n",
    "    # Tokenize full text\n",
    "    tokens = enc.encode(text)\n",
    "\n",
    "    # If shorter than chunk_size, return as-is\n",
    "    if len(tokens) <= chunk_size:\n",
    "        return [text]\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    stride = chunk_size - overlap  # Move forward by this much each time\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "\n",
    "        # Decode back to text\n",
    "        chunk_text = enc.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        # Move to next chunk\n",
    "        if end >= len(tokens):\n",
    "            break\n",
    "        start += stride\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Test it\n",
    "sample = \"This is a test. \" * 1000  # Long text\n",
    "chunks = chunk_text(sample, chunk_size=300, overlap=50)\n",
    "print(f\"Split into {len(chunks)} chunks\")\n",
    "print(f\"First chunk length: {len(tiktoken.get_encoding('cl100k_base').encode(chunks[0]))} tokens\")\n",
    "print(f\"Last chunk length: {len(tiktoken.get_encoding('cl100k_base').encode(chunks[-1]))} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd07ceda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test. This is a test.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51906948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train samples: 181905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181905/181905 [00:29<00:00, 6154.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked train samples: 375678\n",
      "Original test samples: 45478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45478/45478 [00:07<00:00, 6334.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked test samples: 93828\n",
      "\n",
      "Expansion ratio: 2.07x\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def create_chunked_dataset(df: pl.DataFrame, chunk_size: int = 500, overlap: int = 50) -> pl.DataFrame:\n",
    "    \"\"\"Version with document ID tracking.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for doc_id, row in enumerate(tqdm(df.iter_rows(named=True), total=len(df))):\n",
    "        text = row[\"text\"]\n",
    "        label = row[\"label\"]\n",
    "        dataset = row.get(\"dataset\", \"unknown\")\n",
    "\n",
    "        chunks = chunk_text(text, chunk_size=chunk_size, overlap=overlap)\n",
    "\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            rows.append({\n",
    "                \"doc_id\": doc_id,\n",
    "                \"text\": chunk,\n",
    "                \"label\": label,\n",
    "                \"dataset\": dataset,\n",
    "                \"chunk_idx\": chunk_idx,\n",
    "                \"num_chunks\": len(chunks),\n",
    "            })\n",
    "\n",
    "    return pl.DataFrame(rows)\n",
    "\n",
    "\n",
    "# Apply to your data\n",
    "df_train = pl.read_parquet(\"../data/curated_dataset_train.parquet\")\n",
    "df_test = pl.read_parquet(\"../data/curated_dataset_test.parquet\")\n",
    "\n",
    "print(f\"Original train samples: {len(df_train)}\")\n",
    "df_train_chunked = create_chunked_dataset(df_train, chunk_size=300, overlap=50)\n",
    "print(f\"Chunked train samples: {len(df_train_chunked)}\")\n",
    "\n",
    "print(f\"Original test samples: {len(df_test)}\")\n",
    "df_test_chunked = create_chunked_dataset(df_test, chunk_size=300, overlap=50)\n",
    "print(f\"Chunked test samples: {len(df_test_chunked)}\")\n",
    "\n",
    "# Check expansion ratio\n",
    "print(f\"\\nExpansion ratio: {len(df_train_chunked) / len(df_train):.2f}x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is-it-slop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

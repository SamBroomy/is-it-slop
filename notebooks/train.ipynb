{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import onnx\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from __init__ import (\n",
    "    CLASSIFICATION_THRESHOLD_PATH,\n",
    "    MODEL_ONNX_PATH,\n",
    "    PLOT_DIR,\n",
    "    RETRAIN_VECTORIZER,\n",
    "    RETRAINED_MODEL_VERSION,\n",
    "    SEED,\n",
    "    VECTORIZER_BIN_PATH,\n",
    "    VECTORIZER_JSON_PATH,\n",
    "    ProbabilisticClassifier,\n",
    "    df_test,\n",
    "    df_train,\n",
    ")\n",
    "from is_it_slop_preprocessing import TfidfVectorizer, VectorizerParams, __version__\n",
    "from loguru import logger\n",
    "from onnxruntime.transformers.onnx_model import OnnxModel\n",
    "from skl2onnx import to_onnx\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.calibration import CalibratedClassifierCV, LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from .plots import (\n",
    "    analyze_features_by_ngram_length,\n",
    "    artifact_position_analysis,\n",
    "    compare_token_distributions,\n",
    "    compute_best_thresholds,\n",
    "    dataset_bias_analysis,\n",
    "    decision_boundary_analysis,\n",
    "    embedding_visualization,\n",
    "    per_dataset_accuracy_analysis,\n",
    "    plot_calibration_curves,\n",
    "    plot_prediction_distributions,\n",
    "    roc_curve_analysis,\n",
    ")\n",
    "\n",
    "# Python random\n",
    "random.seed(SEED)\n",
    "\n",
    "np.random.default_rng(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"ORT_DETERMINISTIC\"] = \"1\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"is-it-slop-training-pipeline\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"is_it_slop_preprocessing\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.WARNING)\n",
    "print(f\"Bindings version: {__version__}\")\n",
    "print(f\"Pipeline model version output: {RETRAINED_MODEL_VERSION}\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 600\n",
    "plt.rcParams[\"savefig.dpi\"] = 1200\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "\n",
    "print(\"Vectorizer exists:\", VECTORIZER_BIN_PATH.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.select(\"text\").collect().to_series().to_numpy()\n",
    "y_train = df_train.select(\"label\").collect().to_series().to_numpy()\n",
    "\n",
    "X_test = df_test.select(\"text\").collect().to_series().to_numpy()\n",
    "y_test = df_test.select(\"label\").collect().to_series().to_numpy()\n",
    "\n",
    "total_samples = len(X_train) + len(X_test)\n",
    "logger.info(f\"Total samples: {total_samples}\")\n",
    "logger.info(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "# Start MLflow run\n",
    "mlflow.start_run()\n",
    "\n",
    "# Log dataset info\n",
    "mlflow.log_param(\"total_samples\", total_samples)\n",
    "mlflow.log_param(\"train_samples\", len(X_train))\n",
    "mlflow.log_param(\"test_samples\", len(X_test))\n",
    "mlflow.log_param(\"preprocessing_version\", __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Fitting Rust TF-IDF vectorizer...\")\n",
    "t1 = time.time()\n",
    "\n",
    "if RETRAIN_VECTORIZER or not VECTORIZER_BIN_PATH.exists():\n",
    "    logger.info(\"Training new Vectorizer\")\n",
    "    params = VectorizerParams(ngram_range=(2, 4), min_df=50, max_df=0.8)\n",
    "\n",
    "    # Log vectorizer params\n",
    "    mlflow.log_param(\"ngram_range\", f\"{params.ngram_range}\")\n",
    "    mlflow.log_param(\"min_df\", params.min_df)\n",
    "    mlflow.log_param(\"max_df\", params.max_df)\n",
    "    mlflow.log_param(\"retrain_vectorizer\", True)\n",
    "\n",
    "    (vectorizer, X_train_tfidf) = TfidfVectorizer.fit_transform(X_train, params)\n",
    "    logger.info(\n",
    "        f\"Fitted vectorizer and transformed training data {X_train_tfidf.shape} in {time.time() - t1:.2f} seconds\"\n",
    "    )\n",
    "    t2 = time.time()\n",
    "else:\n",
    "    logger.info(\"Loading Pre-trained Vectorizer\")\n",
    "\n",
    "    vectorizer = TfidfVectorizer.load(VECTORIZER_BIN_PATH)\n",
    "    mlflow.log_param(\"retrain_vectorizer\", False)\n",
    "    logger.info(f\"Loaded vectorizer in {time.time() - t1:.2f} seconds\")\n",
    "    t2 = time.time()\n",
    "    X_train_tfidf = vectorizer.transform(X_train)\n",
    "    logger.info(f\"Transformed training data {X_train_tfidf.shape} in {time.time() - t2:.2f} seconds\")\n",
    "    t2 = time.time()\n",
    "logger.info(\"Transforming test data...\")\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "logger.info(f\"Transformed test data {X_test_tfidf.shape} in {time.time() - t2:.2f} seconds\")\n",
    "logger.info(f\"Train Feature matrix: {X_train_tfidf.shape}\")\n",
    "sparsity = 100 * (1 - X_train_tfidf.nnz / np.prod(X_train_tfidf.shape))  # pyright: ignore[reportCallIssue, reportArgumentType]\n",
    "logger.info(f\"Sparsity: {sparsity:.2f}%\")\n",
    "\n",
    "# Log feature matrix metrics\n",
    "mlflow.log_metric(\"n_features\", X_train_tfidf.shape[1])  # pyright: ignore[reportOptionalSubscript]\n",
    "mlflow.log_metric(\"sparsity_percent\", sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ensemble\n",
    "logger.info(\"Training ensemble...\")\n",
    "\n",
    "nb = MultinomialNB(alpha=0.01)\n",
    "\n",
    "# cn = ComplementNB(alpha=0.01)\n",
    "\n",
    "sgd = SGDClassifier(\n",
    "    loss=\"modified_huber\",\n",
    "    penalty=\"l2\",\n",
    "    alpha=0.00005,\n",
    "    class_weight=\"balanced\",\n",
    "    early_stopping=True,\n",
    "    max_iter=8000,\n",
    "    tol=1e-4,\n",
    "    random_state=SEED,\n",
    "    learning_rate=\"optimal\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l2\", C=1.0, solver=\"saga\", max_iter=1000, class_weight=\"balanced\", random_state=SEED, n_jobs=-1\n",
    ")\n",
    "\n",
    "# LinearSVC - very fast, needs calibration for probabilities\n",
    "svc = LinearSVC(\n",
    "    C=1.0,\n",
    "    loss=\"squared_hinge\",  # Good for sparse data\n",
    "    max_iter=2000,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "# Wrap for probability calibration (needed for ensemble voting='soft')\n",
    "svc_calibrated = CalibratedClassifierCV(svc, cv=5, method=\"sigmoid\")\n",
    "\n",
    "estimators: list[tuple[str, BaseEstimator, float]] = [\n",
    "    (\"sgd\", sgd, 0.25),\n",
    "    (\"logreg\", logreg, 0.30),\n",
    "    (\"svc\", svc_calibrated, 0.30),\n",
    "    (\"nb\", nb, 0.15),\n",
    "    # (\"cnb\", cn, 0.05),\n",
    "]\n",
    "voting = \"soft\"\n",
    "\n",
    "assert abs(sum(weight for _, _, weight in estimators) - 1.0) < 1e-6, \"Weights must sum to 1.0\"  # noqa: S101\n",
    "\n",
    "mlflow.log_param(\"ensemble_estimators\", [name for name, _, _ in estimators])\n",
    "mlflow.log_param(\"ensemble_weights\", [weight for _, _, weight in estimators])\n",
    "mlflow.log_param(\"model_type\", \"VotingClassifier\")\n",
    "mlflow.log_param(\"voting\", voting)\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[(name, model) for name, model, _ in estimators],\n",
    "    weights=[weight for _, _, weight in estimators],\n",
    "    voting=voting,\n",
    "    n_jobs=-1,\n",
    "    flatten_transform=False,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Retrain\n",
    "ensemble.fit(X_train_tfidf, y_train)\n",
    "# This is just a list but to save to onnx we need it as a numpy array\n",
    "ensemble.weights = np.array(ensemble.weights)  # pyright: ignore[reportAttributeAccessIssue]\n",
    "\n",
    "# Use a Protocol or Union type for classifiers with predict_proba\n",
    "\n",
    "\n",
    "models: dict[str, ProbabilisticClassifier] = {\n",
    "    \"sgd\": ensemble.estimators_[0],\n",
    "    \"logreg\": ensemble.estimators_[1],\n",
    "    \"svc\": ensemble.estimators_[2],\n",
    "    \"nb\": ensemble.estimators_[3],\n",
    "    # \"cnb\": ensemble.estimators_[4],\n",
    "    \"ensemble\": ensemble,\n",
    "}  # pyright: ignore[reportAssignmentType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train_tfidf.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = ensemble.predict_proba(X_test_tfidf)[:, 1]\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_analysis(X_train_tfidf, y_train, X_test_tfidf, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) use precision-recall curve for exact best F1 (thresholds length differs)\n",
    "\n",
    "\n",
    "best_threshold, best_threshold_roc = compute_best_thresholds(y_test, y_probs)\n",
    "\n",
    "# Log thresholds\n",
    "mlflow.log_metric(\"best_threshold_f1\", best_threshold)\n",
    "mlflow.log_metric(\"best_threshold_youden\", best_threshold_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_probs >= best_threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 is best, 0 is random, -1 is worst\n",
    "test_mcc = matthews_corrcoef(y_test, y_pred)\n",
    "logger.info(f\"Validation MCC: {test_mcc:.4f}\")\n",
    "\n",
    "test_auc: float = roc_auc_score(y_test, y_pred)  # pyright: ignore[reportAssignmentType]\n",
    "logger.info(f\"Validation AUC: {test_auc:.4f}\")\n",
    "accuracy: float = accuracy_score(y_test, y_pred)  # pyright: ignore[reportAssignmentType]\n",
    "logger.info(f\"Accuracy:   {accuracy:.4f}\")\n",
    "precision: float = precision_score(y_test, y_pred)  # pyright: ignore[reportAssignmentType]\n",
    "logger.info(f\"Precision:  {precision:.4f}\")\n",
    "recall: float = recall_score(y_test, y_pred)  # pyright: ignore[reportAssignmentType]\n",
    "logger.info(f\"Recall:     {recall:.4f}\")\n",
    "f1: float = f1_score(y_test, y_pred)  # pyright: ignore[reportAssignmentType]\n",
    "logger.info(f\"F1 Score:   {f1:.4f}\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "logger.info(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n",
    "logger.info(\"Confusion Matrix:\")\n",
    "logger.info(\"              Predicted\")\n",
    "logger.info(\"                 0      1\")\n",
    "logger.info(f\"Actual  0    {tn:5d}  {fp:5d}\")\n",
    "logger.info(f\"        1    {fn:5d}  {tp:5d}\")\n",
    "dis = ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "plot_path = PLOT_DIR / \"confusion_matrix.png\"\n",
    "dis.figure_.savefig(plot_path, bbox_inches=\"tight\")\n",
    "mlflow.log_artifact(str(plot_path))\n",
    "\n",
    "# Log all metrics to MLflow\n",
    "mlflow.log_metric(\"test_mcc\", test_mcc)\n",
    "mlflow.log_metric(\"test_auc\", test_auc)\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "mlflow.log_metric(\"precision\", precision)\n",
    "mlflow.log_metric(\"recall\", recall)\n",
    "mlflow.log_metric(\"f1_score\", f1)\n",
    "mlflow.log_metric(\"true_positives\", int(tp))\n",
    "mlflow.log_metric(\"false_positives\", int(fp))\n",
    "mlflow.log_metric(\"true_negatives\", int(tn))\n",
    "mlflow.log_metric(\"false_negatives\", int(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save vectorizer in both formats:\n",
    "# 1. JSON-wrapped format for Python (with metadata)\n",
    "vectorizer.save(VECTORIZER_JSON_PATH)\n",
    "logger.info(f\"Saved json vectorizer to {VECTORIZER_JSON_PATH}\")\n",
    "# 2. Raw bincode format for Rust (no JSON wrapper)\n",
    "vectorizer.save(VECTORIZER_BIN_PATH)\n",
    "logger.info(f\"Saved binary vectorizer to {VECTORIZER_BIN_PATH}\")\n",
    "\n",
    "Path(CLASSIFICATION_THRESHOLD_PATH).write_text(str(best_threshold), encoding=\"utf-8\")\n",
    "logger.info(f\"Saved classification threshold to {CLASSIFICATION_THRESHOLD_PATH}\")\n",
    "# Convert to ONNX\n",
    "# Disable ZipMap to output probabilities as a 2D tensor [batch_size, num_classes]\n",
    "onx: onnx.ModelProto = to_onnx(\n",
    "    ensemble,\n",
    "    X_train_tfidf[:1].toarray(),  # Sample for shape inference\n",
    "    options={\n",
    "        type(ensemble): {\"zipmap\": False}  # Output probabilities as tensor, not dict\n",
    "    },\n",
    ")  # pyright: ignore[reportAssignmentType]\n",
    "onnx.checker.check_model(onx, full_check=True)\n",
    "\n",
    "\n",
    "# with MODEL_ONNX_PATH.open(\"wb\") as f:\n",
    "#     f.write(onx.SerializeToString())#deterministic=True))\n",
    "# logger.info(f\"Saved ONNX model to {MODEL_ONNX_PATH}\")\n",
    "\n",
    "# onnx_model = onnx.load(MODEL_ONNX_PATH)\n",
    "\n",
    "\n",
    "# To get rid of the following errors we need to prune the graph\n",
    "# \"CleanUnusedInitializersAndNodeArgs] Removing initializer 'classes_ind'. It is not used by any node and should be removed from the model\"\n",
    "onnx_model = OnnxModel(onx)\n",
    "onnx_model.prune_graph()\n",
    "onnx_model.save_model_to_file(MODEL_ONNX_PATH)\n",
    "\n",
    "\n",
    "# Log artifacts to MLflow\n",
    "mlflow.log_artifact(str(MODEL_ONNX_PATH))\n",
    "mlflow.log_artifact(str(VECTORIZER_BIN_PATH))\n",
    "mlflow.log_artifact(str(VECTORIZER_JSON_PATH))\n",
    "mlflow.log_artifact(str(CLASSIFICATION_THRESHOLD_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "sess = rt.InferenceSession(MODEL_ONNX_PATH, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "test_input = X_train_tfidf[:2]  # .astype(np.float64)  # .todense()\n",
    "\n",
    "input_name = sess.get_inputs()[0].name\n",
    "\n",
    "pred_onx = sess.run(None, {input_name: test_input.toarray()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pred = ensemble.predict_proba(test_input)\n",
    "model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(pred_onx[1], model_pred)  # pyright: ignore[reportArgumentType]  # noqa: S101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_distributions(X_test_tfidf, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_calibration_curves(X_test_tfidf, y_test, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_boundary_analysis(\n",
    "    X_test_tfidf, y_test, ensemble.predict_proba(X_test_tfidf), decision_threshold=best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_features_by_ngram_length(vectorizer, models, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bias_analysis(df_test.collect().to_pandas(), ensemble.predict_proba(X_test_tfidf), best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_visualization(\n",
    "    X_test_tfidf, y_test, df_test.select(\"dataset\").collect().to_series().to_numpy(), sample_size=20_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_human = df_test.filter(pl.col(\"label\") == 0).select(\"text\").collect().to_series()\n",
    "texts_ai = df_test.filter(pl.col(\"label\") == 1).select(\"text\").collect().to_series()\n",
    "compare_token_distributions(texts_human, texts_ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_position_analysis(\n",
    "    df_test.select(\"text\").collect().to_series().to_list(), y_test, vectorizer,\n",
    "    ensemble,  # type: ignore[reportArgumentType]\n",
    "    best_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_dataset_accuracy_analysis(X_test_tfidf, models[\"svc\"], threshold=best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End MLflow run\n",
    "mlflow.end_run()\n",
    "logger.info(\"MLflow run completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is-it-slop-package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
